\documentclass[10pt]{article}
\pdfoutput=1 

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}

\openup 1em

%macro for commenting
\usepackage{color}
\newcommand{\leo}[1]{{\color{blue}{\it leo: #1}}}

\newcommand{\Xbeta}{ X_i \theta}
\newcommand{\xbeta}{ x_i \theta}
\newcommand{\xbetaij}{ x_{ij}^T \theta}
\newcommand{\sgamma}{s_{ij}^T\gamma_i}

\usepackage[round]{natbib}

\usepackage{rotating}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{float}


\usepackage{amsthm,amsmath} 
\usepackage{amssymb}
\usepackage{subcaption}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}


\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{mhequ}
\newcommand{\be}{\begin{equs}}
\newcommand{\ee}{\end{equs}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\DeclareMathOperator{\Binom}{Binomial}
\DeclareMathOperator{\No}{No}
\DeclareMathOperator{\PG}{PG}
\DeclareMathOperator{\Bern}{Bernoulli}
\DeclareMathOperator{\U}{Uniform}
\DeclareMathOperator{\NB}{NB}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Diag}{Diag}
\DeclareMathOperator{\bigO}{\mc O}
\newcommand{\James}[1]{\textcolor{blue}{#1}}


\thispagestyle{empty}
\baselineskip=28pt

\title
{{Calibrated Data Augmentation for Scalable \\ Markov Chain Monte Carlo}}


\author{
     Leo L. Duan,
     James E. Johndrow,
     David B. Dunson
    % \textsuperscript{*}\footnotemark[2]\and
}

 
\begin{document}
    
\maketitle

% generally I don't use abbreviations in an abstract; some journals don't allow it
{\bf Abstract:} Data augmentation is a common technique for building tuning-free Markov chain Monte Carlo algorithms. Although these algorithms are very popular, 
autocorrelations are often high in large samples, leading to poor computational efficiency.  This phenomenon has been attributed to a discrepancy between Gibbs step sizes and the rate of posterior concentration in large samples.  In this article, we propose a family of calibrated data augmentation algorithms, which adjust for this discrepancy by inflating Gibbs step sizes with an auxiliary parameter. The bias introduced by the scale parameter can be eliminated through a Metropolis-Hastings step. The approach is applicable to a broad variety of existing data augmentation algorithms, and we focus on three popular models: probit, logistic and Poisson log-linear.  Theoretical support is provided and dramatic gains are shown in applications.
\vskip 12pt

%\baselineskip=12pt
%\par\vfill\noindent
{\noindent  KEY WORDS:  Bayesian probit; Bayesian logit; Big $n$; Data Augmentation; Maximal Correlation; Polya-Gamma.}
%\par\medskip\noindent
%\clearpage\pagebreak\newpage
\pagenumbering{arabic}

\section{Introduction}

With the deluge of data in many modern application areas, there is pressing need for scalable computational algorithms for inference from such data, including uncertainty quantification (UQ).  Somewhat surprisingly, even as the volume of data increases, uncertainty often remains sizable.  Examples in which this phenomenon occurs include financial fraud detection \citep{ngai2011application}, disease mapping \citep{wakefield2007disease} and online click-through tracking \citep{wang2010click}.  Bayesian approaches provide a useful paradigm for quantifying uncertainty in inferences and predictions in these and other settings.

The standard approach to Bayesian posterior computation is Markov chain Monte Carlo (MCMC) and related sampling algorithms. Non-sampling alternatives, such as variational Bayes, tend lack general accuracy guarantees. However, it is well known that conventional MCMC algorithms often scale poorly in problem size and complexity. Due to its sequential nature, the computational cost of MCMC is the product of two factors: the evaluation cost at each sampling iteration and the total number of iterations needed to obtain an acceptably low Monte Carlo error. The latter is related to the properties of the Markov transition kernel; we will refer to this informally as the \emph{mixing properties} of the Markov chain. 

In recent years, a substantial literature has developed focusing on decreasing computational cost per iteration (\cite{minsker2014robust,srivastava2015wasp,conrad2015accelerating} among others), mainly through accelerating or parallelizing the sampling procedures at each iteration. Moreover, myriad strategies for improving mixing have been described in the literature. For Metropolis-Hastings (M-H) algorithms, improving mixing is usually a matter of contructing a better proposal distribution. An important difference between M-H and Gibbs is that one has direct control over step sizes in M-H through choice of the proposal, while Gibbs step sizes are generally not tunable; on the other hand, finding a good proposal for multi-dimensional parameters in M-H is significantly more challenging compared to Gibbs sampling. Thus, improving mixing for Gibbs has historically focused on decreasing autocorrelation by changing the update rule itself, for example by parameter expansion (PX), marginalization, or slice sampling.\footnote{Although strictly speaking, slice sampling is just an alternative approach to sampling from a full conditional distribution, in practice, it is often an alternative to data augmentation, so that using a slice sampling strategy results in the removal of a data augmentation step from an alternative Gibbs sampler.} 

The theory literature on behavior of MCMC for large $n$ and/or $p$ is arguably somewhat limited. Many authors have focused on studying mixing properties by showing 
a general ergodicity condition, such as geometric ergodicity \citep{roberts2004general,meyn2012markov}. This generally yields bounds on the convergence rate and spectral gap of the Markov chain, but \cite{rajaratnam2015mcmc} observe that in many cases, these bounds converge to zero exponentially fast in $p$ or $n$, so that no meaningful guarantee of performance for large problem sizes is provided by most existing bounds. In the probability literature, a series of papers have developed an analogue of Harris' theorem and ergodic theory for infinite-dimensional state spaces \citep{hairer2011asymptotic}. Recent work verifies the existence of MCMC algorithms for computation in differential equation models with dimension-independent spectral gap \citep{hairer2014spectral}. In this example, the algorithm under consideration is an M-H algorithm, and it is clear that the proposal must be tuned very carefully to achieve dimension independence. Other work has studied the properties of the limiting differential equation that describes infinite-dimensional dynamics of MCMC.

A recent paper (\cite{johndrow2016inefficiency}) studies popular data augmentation algorithms for posterior computation in probit \citep{albert1993bayesian} and logistic  \citep{polson2013bayesian} models, showing that the algorithms fail to mix in large sample sizes when the data are imbalanced. An important insight is that the performance can be largely explained by a discrepancy between the rate at which Gibbs step sizes and the width of the high-probability region of the posterior converge to zero as the sample size increases. Thus, since Gibbs step sizes are generally not tunable, slow mixing is likely to occur as the sample size grows unless the order of the step size happens to match the order of the posterior width. This implies that if a way to directly control the step sizes of the Gibbs sampler could be devised, it would be possible to make the mixing properties of the sampler insensitive to sample size by scaling the step sizes appropriately. This is similar to the conclusion of \cite{hairer2014spectral}, except in this case, we have growing $n$ instead of growing $p$.

In this article, we propose a method for tuning Gibbs step sizes by introducing auxiliary parameters that change the variance of full conditional distributions for one or more parameters. Although we focus on data augmentation algorithms for logit, probit, and Poisson log-linear models, in principle the strategy can be applied more generally to align Gibbs step sizes with the size of the space being explored. As these ``calibrated'' data augmentation algorithms alter the invariant measure, one can use the Gibbs step as a highly efficient M-H proposal, thereby recovering the correct invariant, or view the resulting algorithm as a perturbation of the original Markov chain.  In this article, we focus on the former strategy, providing theoretical support and showing very substantial practical gains in computational efficiency attributed to our calibration approach.

\section{Calibrated Data Augmentation}

Our method is developed primarily in the context of data augmentation Gibbs samplers, which are based on the integral $ \pi(\theta|y)=\int f(\theta|z,y) \pi(z|y) dz$, where $\theta$ are model parameters, $y$ denotes observed data, and $z$ denotes latent data included for computational purposes.  Data augmentation Gibbs samplers alternate between sampling the latent data $z$ from their conditional posterior distribution given $\theta$ and $y$, and sampling parameters $\theta$ given the latent $z$ and observed data $y$; either of these steps can be further broken down into a series of full conditional sampling steps but we focus for simplicity on algorithms of the form: 
\be \label{eq:da}
z \mid \theta, y &\sim \pi(z;\theta,y) \\
\theta \mid z,y &\sim f(\mu(z),\Sigma(z)),
\ee
where $f$ belongs to a location-scale family, such as the Gaussian.  Popular data augmentation algorithms are designed so that both of these sampling steps can be conducted easily and efficiently; e.g., sampling the latent data for each subject independently and then drawing $\theta$ simultaneously (or at least in blocks) from a multivariate Gaussian or other standard distribution.  This effectively avoids the need for tuning, which is a major issue for Metropolis-Hastings algorithms, particularly when $\theta$ is high-dimensional.
Data augmentation algorithms are particularly common for generalized linear models (GLMs), with $\bb E(y_i \mid x_i, \theta) = g^{-1}(x_i \theta)$ and a conditionally Gaussian prior distribution chosen for $\theta$. We focus in particular on Poisson log-linear, binomial logistic, and binomial probit as motivating examples.

\subsection{Initial example: Probit with improper prior}
We introduce our calibration approach through a binomial probit model example: 
\be
y_i \sim \Bern(p_i), \quad p_i = \Phi(x_i \theta),
\ee
with improper prior $\pi(\theta) \propto 1$. The basic data augmentation algorithm \citep{tanner1987calculation,albert1993bayesian} has the update rule
\be
z_i \mid \theta, x_i, y_i &\sim \left\{ \begin{array}{cc} \No_{[0,\infty)}(x_i \theta,1) & \text{ if } y_i = 1 \\ \No_{(-\infty,0]}(x_i \theta,1) & \text{ if } y_i = 0 \end{array} \right. \\
\theta \mid z, x, y &\sim \No((X'X)^{-1} X'z, (X'X)^{-1}),
\ee
where $\No_{[a,b]}(\mu,\sigma^2)$ is the normal distribution with mean $\mu$ and variance $\sigma^2$ truncated to the interval $[a,b]$.  %\cite{johndrow2016inefficiency} found that this algorithm has very poor mixing in imbalanced large data settings.  To solve this problem, 
We propose to make the Gibbs step sizes tunable by introducing an auxiliary parameter $r_i$ multiplying the variance of $z_i$, while also reducing the bias caused by $r_i$ through adjusting the mean by another auxiliary parameter $b_i$.  These adjustments yield 
\be
\mbox{pr}(y_i = 1 | \theta, x_i, r_i, b_i) = \int_{0}^{\infty} \frac{1}{\sqrt{2 \pi r_i} } \exp\left(-\frac{(z_i-x_i\theta-b_i)^2}{2 r_i^2} \right) dz_i = \Phi\bigg( \frac{x_i\theta+b_i}{\sqrt{r_i}}\bigg),
\label{eq:prop-marginal-probit}
\ee
which generalizes $\mbox{pr}(y_i=1 | \theta, x_i) = \Phi( x_i\theta )$ leading to the modified data augmentation algorithm
\be \label{eq:cda-probit}
z_i \mid \theta, x_i, y_i &\sim \left\{ \begin{array}{cc} \No_{[0,\infty)}(x_i \theta+b_i,r_i) & \text{ if } y_i = 1 \\ \No_{(-\infty,0]}(x_i \theta+b_i,r_i) & \text{ if } y_i = 0 \end{array} \right. \\
\theta^* \mid z, X &\sim \No((X'R^{-1}X)^{-1} X'R^{-1}(z-b), (X'R^{-1}X)^{-1}),
\ee
where $R = \diag(r_1,\ldots,r_n)$, $b = (b_1,\ldots,b_n)'$.  This differs fundamentally from the parameter expansion algorithms of \cite{liu1999parameter} and \cite{meng1999seeking} that rescale $\theta$ by $1/\sqrt{r}$, which does not impact the conditional variance of $\theta$ and so does not solve the mis-calibration problem.

The update in \eqref{eq:cda-probit} alters the invariant measure from $\pi(\theta |y)$ to $\pi^*(\theta | y)$, and hence the Gibbs samples for $\theta$ will not be exactly from $\pi( \theta | y)$ even after convergence.  To adjust for the bias caused by the difference between $\pi(\theta|y)$ and $\pi^*(\theta|y)$, we use (\ref{eq:cda-probit}) as an M-H proposal.  Letting $Q(\theta^*;\theta) = \int f(\theta^*|z)  \pi(z|\theta) dz $ be the proposal defined by \eqref{eq:cda-probit} marginalized over $z$, the proposal is accepted with probability
%Generate one uniform random variable $u\sim \U(0,1)$ and accept the new $\theta^*$ if
\be
1 \wedge \frac{Q(\theta; \theta^*) \pi(\theta^*) \prod_i L(\xbeta^*;y_i)}{Q(\theta^*; \theta)\pi(\theta) \prod_i L(\xbeta;y_i) } = 1 \wedge  \frac{  \prod_i L_r(\xbeta;y_i) L(\xbeta^*;y_i)}{\prod_i  L_r(\xbeta^*;y_i)L(\xbeta;y_i) },
\label{eq:mh-criterion}
\ee
where $L(\eta_i;y_i)=\Phi(\eta_i)^{y_i} (1-\Phi(\eta_i))^{(1-y_i)}$ and  $L_r(\eta_i;y_i)=\Phi( \frac{\eta_i+b_i}{\sqrt{r_i}} )^{y_i} (1-\Phi(\frac{\eta_i+b_i}{\sqrt{r_i}}))^{(1-y_i)}$. The second equality holds since $Q(\theta; \theta^*) Q(\theta^*) = Q(\theta; \theta^*)Q(\theta)$ and $ Q(\theta) = C\pi(\theta) \prod_i L_r(x_i\theta;y_i)$, which is the posterior density under the altered $L_r$ with $C$ a constant. Setting $r_i=1$ and $b_i=0$ leads to acceptance rate of $1$, which corresponds to the original Gibbs sampling step.

At the iteration $t$, when the proposal is accepted $\theta_t = \theta^*$, the covariance:
\be
\cov(\theta_t \mid \theta_{t-1}, r,X,z) = (X'R^{-1}X)^{-1} + (X'R^{-1}X)^{-1} X'R^{-1}\cov(z-b | R) R^{-1}X(X'R^{-1}X)^{-1},
\ee
so that the step size is equal to 
\be
 \var(\theta_t \mid \theta_{t-1}, r,X,z)   \ge  \diag((X'R^{-1}X)^{-1}), \label{eq:varlb-probit}
\ee
with the lower bound a simple function of the $r_i$s.  Mis-calibration of the usual data augmentation algorithm, which sets $r_i=1,b_i=0$, occurs when the step size in (\ref{eq:varlb-probit}) decreases at a faster rate in $n$ and/or $p$ than the posterior $\pi( \theta | y)$ unconditionally on the augmented data $z$.  The key to calibrated data augmentation (CDA) is to choose $r,b$ to minimize or eliminate this mis-calibration while additionally maximizing the M-H acceptance probability, which is similar to 
minimizing the discrepancy between $\pi^*(\theta | y)$ and 
$\pi(\theta | y)$.  Before describing a general algorithm to estimate $r,b$, we illustrate how CDA can be used to address the problem with DA introduced by 
\cite{johndrow2016inefficiency}.  

\subsection{Imbalanced data intercept only case}

In an intercept-only model, the variance is bounded by $\left(\sum_i r_i^{-1}\right)^{-1}$ via \eqref{eq:varlb-probit}, which is $1/n$ times the harmonic mean of the $r_i$s. \cite{johndrow2016inefficiency} show that when $\sum_i y_i = 1$ and $r_i = 1$, $\var(\theta_t \mid \theta_{t-1})$ is approximately $n^{-1} \log n$, while the width of the high probability region of the posterior is order $(\log n)^{-1}$, leading to slow mixing. To achieve step sizes consistent with the width of the high posterior probability region, we need
\be
\left(\sum_i r_i^{-1}\right)^{-1} &\approx (\log n)^{-1},
\ee
so if $r_i = r$ for all $i$, $r \approx n/\log n$.

To illustrate the effect of this calibration, consider an intercept only probit model, with $\sum_i y_i =1$ and $n=10^4$. Setting $r=1$ in the proposal corresponds to the original \cite{albert1993bayesian} Gibbs sampler, which suffers from extremely slow mixing in this case.  Letting $r = n/\log n$ to calibrate the sampler, we then choose the $b_i$'s to increase the acceptance rate in the M-H step; as illustration we simply let $b_i = -3.7 (\sqrt r -1)$ to induce $\mbox{pr}(y_i = 1) = \Phi(-3.7) = n^{-1}\sum_i y_i = 10^{-4}$ in the proposal distribution.  Later we will propose a method for estimating the $r_i$s and $b_i$s.  

We ran our CDA Gibbs sampler for these data and different values of $r$, ranging from $r=1$ for uncalibrated data augmentation to $r=5,000$, with $r=1,000 \approx n/\log n$ corresponding to our recommended default value.  Figure~\ref{probit_demo_intercept_proposal} plots autocorrelation functions (ACFs) for these different samplers without  M-H adjustment. Autocorrelation is very high even at lag 40 for the uncalibrated sampler ($r=1$), indicating extremely poor mixing.  Increasing $r$ leads to dramatic improvements in mixing, but there are no further gains in increasing $r$ from our recommend default value to $r=5,000$. Figure~\ref{probit_demo_intercept_density} shows kernel-smoothed density estimates of the posterior of $\theta$ without M-H adjustment for different values of $r$ and based on long chains to minimize the impact of Monte Carlo error on differences in the estimates; it is apparent that the density estimates change with $r$. Therefore, for the target distribution with $r=1$, the M-H adjustment that proposes form one with increased $r$, small increase in $r$ (e.g. $r=10, 100$) retains a close to $1$ acceptance rate but little improvement in mixing, large $r$ results in smaller acceptance rate ($0.6$ for $r=1,000$, $0.2$ for $r=5,000$)  but significant improvement in mixing (Figure~\ref{probit_demo_intercept_posteriorsample}).





%it is apparent that all of the density estimates are very similar.  This suggests that the M-H adjustment has little impact in this case, with $\pi(\theta | y) \approx \pi^*(\theta | y)$ regardless of $r$. 

\begin{figure}[H]
 % \centering
  \begin{subfigure}[b]{0.32\textwidth}
 \includegraphics[width=1\textwidth]{probit_demo_acf_prop.pdf}
  \caption{ACF for CDA without M-H adjustment.}
 \label{probit_demo_intercept_proposal}
\end{subfigure}
  \hfill
    \begin{subfigure}[b]{0.32\textwidth}
 \includegraphics[width=1\textwidth]{density_probit.pdf}
  \caption{Density estimates of the posterior without M-H adjustment.}
   \label{probit_demo_intercept_density}
\end{subfigure}
\hfill
   \begin{subfigure}[b]{0.32\textwidth}
 \includegraphics[width=1\textwidth]{probit_demo_acf.pdf}
  \caption{ACF for CDA with M-H adjustment}
   \label{probit_demo_intercept_posteriorsample}
\end{subfigure}
  \hfill
  \caption{ Autocorrelation functions (ACFs) and kernel-smoothed density estimates for different CDA samplers in intercept-only probit model.}
 \label{probit_demo_intercept}
 \end{figure}


\subsection{Choice of $r$ and $b$ using Asymptotic Approximation}

As illustrated in the previous subsection, efficiency of CDA is dependent on a good choice of the auxiliary parameters $r=(r_1,\ldots,r_n)$ and $b=(b_1,\ldots,b_n)$. In general, it is not straightforward to analyze the exact difference between the conditional and marginal variances, however, one can compute the two Fisher information matrices based on conditional and marginal posteriors, and use their inverse as the asymptotic approximates.

Continuing on the probit regression example with the linear predictor $\eta_i = x_i\theta$, the Fisher information based on the marginal and the conditional posteriors given $z_i$ are:

\be
X' \diag\{\frac{\phi(\eta_i)^2}{ {\Phi(\eta_i)(1- \Phi(\eta_i))}}\} X, \quad X' R^{-1} X
\ee
respectively, where $\phi$ is the standard normal density. Therefore, setting $r_i = \frac{\Phi(\eta_i)(1- \Phi(\eta_i))} {\phi(\eta_i)^2}$ completely calibrates the difference between the two.

Then we change $b$ to increase the acceptance rate. Since the acceptance probability at each step is $1\wedge \prod_i \frac{   L_r(\eta_i;y_i) L(\eta_i^*;y_i)}{ L_r(\eta_i^*;y_i)L(;y_i) }$. Given the values of $r_i$ and $\eta_i$, one choice for $b_i$ is the analytical solution that makes $ L_r(\eta_i;y_i) = L(\eta_i;y_i)$. In the probit example, this is  $b_i = \eta_i (\sqrt{r_i}-1)$.

Since $\theta$ and $\eta$ are not known before sampling, we use a short tuning period to sample them and update $r$ and $b$ at the end of each iteration. The acceptance rate can be monitored as the number of acceptance divided by the total tuning iterations. After the rate reaches reaches satisfactory level (e.g. $0.3$), we stop the adaption and keep $r$ and $b$ fixed. Then the algorithm goes through burn-in and posterior sampling like ordinary MCMC.

To illustrate, we consider a probit regression with an intercept and two predictors $x_{i,1},x_{i,2}\sim \No(1,1)$, with $\theta=\{-5,1,-1\}'$, generating $\sum y_i=20$ among $n=10,000$. The \cite{albert1993bayesian} DA algorithm mixes slowly (Figure~\ref{probit_reg_trace} and \ref{probit_reg_acf}). To compare with an early method, we test the parameter expansion algorithm (PX-DA) proposed by \cite{liu1999parameter}. PX-DA only mildly reduces the correlation, as it does not solve the variance mismatch problem.

We started CDA with the adaptive algorithm for 100 iterations, obtaining an acceptance rate of $0.43$, then ran $100$ steps as burn-in and collected the posterior sample for $500$ steps. The calibrated algorithm solves the mixing problem (Figure~\ref{probit_reg_trace} and \ref{probit_reg_acf}).

 
\begin{figure}[H]
 % \centering
  \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{probit15_trace_plot.pdf}
  \caption{Traceplot for the original DA, parameter expanded DA and CDA algorithms.}
  \label{probit_reg_trace}
\end{subfigure}
  \hfill
   \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{probit15_acf.pdf}
  \caption{ACF for original DA, parameter expanded DA and CDA algorithms.}
    \label{probit_reg_acf}
\end{subfigure}
 \caption{Panel (a) demonstrates in traceplot and panel (b) in autocorrelation about the substantial improvement in CDA by correcting the variance mis-match in probit regression with rare event data, compared with the original \citep{albert1993bayesian} and parameter-expanded methods \citep{liu1999parameter}.}
 \end{figure}

\subsection{Calibrating Random Conditional Variance via Latent Variable}

It is easy to obtain good acceptance rate in CDA, because the increase in conditional variance does not affect  the existence of marginal closed-form, and the calibrated marginal is close to the original one.

 In the previous examples, as $\mbox{var}(\theta|z,y)$ does not involve the random latent variable $z$, one can directly use $r$ on it to increase the conditional variance. On the other hand, when $\mbox{var}(\theta|z,y)$ depends on the random value of $z$, including $r$ in it is effective and  could cause difficulty in integration $\pi(\theta|y)=\int f(\theta|z,y)\pi(z|y) dz$. Therefore, in this section we show a different strategy that modifies $\pi(z|y)$ to increase $\bb E_z \mbox{var}(\theta|z,y)$.

To illustrate, consider the logistic regression:

\be
y_i \sim \Bern(p_i), \quad p_i = \frac{\exp(x_i \theta)}{1+\exp(x_i \theta)},
\ee
and improper prior $\pi(\theta)=1$. The Polya-Gamma data augmentation has the update rule \citep{polson2013bayesian}:

\be
 z_i &\sim {\PG}(1, |\xbeta|),\\
\theta &\sim \No \left(  (X' Z X)^{-1}   X'  (y-\frac{1}{2})  ,  (X' Z X)^{-1}  \right),
\ee
where $Z= \diag(z_1,\ldots,z_n)$. This algorithm is derived based on $L(y_i \mid \xbeta)=  \int \exp\{ \xbeta (y_i-1/2)\} \exp(-\frac{z_i (\xbeta)^2}{2}) \PG(z_i \mid 1,0) dz_i$. As described above, multiplying $r_i$ on $z_i$ is ineffective and makes the integration intractable.

Instead, observing the value of $z$ can be influenced by the first parameter in $\PG(a_1,a_2)$, $\bb{E}z_i= \frac{a_1}{2 a_2}\tanh(\frac{a_2}{2})$, we replace $\PG(z_i \mid 1,0)$ with $\PG(z_i \mid r_i,0)$. Smaller $r_i$ can lead to larger  $\bb E_z \mbox{var}(\theta|z,y)$.


It can be verified the integration is still tractable, applying the mean adjusting term $b_i$ on $\xbeta$, leading to:

\be
L_r(\xbeta;y_i) = & \int_{0}^{\infty}  \exp\{ (\xbeta+b_i) (y_i-r_i/2)\} \exp(-\frac{z_i (\xbeta+b_i)^2}{2}) \PG(z_i \mid r_i,0) dz_i \\
= &  \frac{\exp \{ (x_i \theta + b_i)y_i \}}{\{1+\exp(\xbeta +b_i)\}^{r_i}},
\label{eq:prop-marginal-logit}
\ee
and the update rule for the proposal:

\be
 z_i &\sim {\PG}(r_i, |\xbeta+b_i|),\\
\theta^* &\sim \No \left(  (X' Z X)^{-1}  X'  (y -r_i/2- Zb) ,  (X' Z X)^{-1}  \right),
\ee
with acceptance probability:


\be
1 \wedge  \frac{  \prod_i L_r(\xbeta;y_i) L(\xbeta^*;y_i)}{\prod_i  L_r(\xbeta^*;y_i)L(\xbeta;y_i) } =1 \wedge \prod_i  \frac{ \{1+\exp(\xbeta)\}   \{1+\exp(\xbeta^*+b_i)\}^{r_i} } {  \{1+\exp(\xbeta^*)\}  \{1+\exp(\xbeta+b_i)\}^{r_i}    } 
,
\ee
where $L(\theta;y_i)=\frac{\exp(\theta y_i)}{1+\exp(\theta)}$.

To see exactly why the smaller $r_i$ leads to larger $\bb E_z (X' Z X)^{-1}$, we compute the  first negative moment of the Polya-Gamma distribution. Combining \cite{cressie1981moment} and \cite{polson2013bayesian}, $\bb{E}z_i^{-1}= \int_0^{\infty} \prod_{k=1}^{\infty} (1+ d_k^{-1} t) ^{-r_i} dt$ with $d_k=2(k-\frac{1}{2})^2\pi^2 + \frac{(x_i\theta+b_i)^2}{2}$.

For choosing $r$ during tuning, we compare the two Fisher information matrices based on the marginal and conditional; for the latter, since it depends on $z_i$, we marginalize it out by taking its expectation.

\be
X' \diag\{\frac{\exp(\xbeta)}{ \{1+\exp(\xbeta) \} ^2}\} X, \quad X'  \diag\{ \frac{r_i}{2 |\xbeta+b_i|}\tanh(\frac{|\xbeta+b_i|}{2})\} X
\ee

To correct the difference, we choose $r_i$ to be $\frac{\exp(\xbeta)}{ \{1+\exp(\xbeta)\} ^2} {2 |\xbeta+b_i|}/ \tanh(\frac{|\xbeta+b_i|}{2})$. To optimize the acceptance rate, given the value of $r_i$ and $\xbeta$,  setting $ \{1+\exp(\xbeta)\}  = \{1+\exp(\xbeta+b_i)\}^{r_i}  $ yields an analytical choice for $b_i$ as $\log[  \{1+\exp(\xbeta)\}^{1/r_i} -1] - \xbeta$ at the end of each tuning iteration.


As a numerical illustration, we use a two parameter intercept-slope model with $x_1\sim \No(0,1)$ and $\theta=\{-9,1\}$. With $n= 10^5$, it leads to rare positive outcome $\sum y_{i} = 50 $. To compare, we ran the original DA  \citep{polson2013bayesian} and the M-H with simple multivariate normal proposal $\theta^*|\theta \sim \No(\theta^*| \theta, {\mc I}^{-1}(\theta))$, with ${\mc I}(\theta)$ being the Fisher information matrix based on the marginal posterior. For CDA we tuned the $r$ and $b$ for $100$ steps, reaching an acceptance rate of $0.8$; then stop adaption and run $100$ and $500$ steps for burn-in and the posterior collecting. Shown in Figure~\ref{logit_random_mixing}, both DA and M-H with normal proposal mix slowly, exhibiting strong autocorrelation even after $40$ lags; whereas CDA substantially accelerates the mixing.

\begin{figure}[H]
 % \centering
  \begin{subfigure}[b]{0.49\textwidth}
 \includegraphics[width=1\textwidth]{logit_demo_trace_plot}
  \caption{Traceplots for the DA and CDA algorithms, and the M-H algorithm with multivariate normal proposal.}
\end{subfigure}
  \hfill
   \begin{subfigure}[b]{0.49\textwidth}
 \includegraphics[width=1\textwidth]{logit_demo_acf}
  \caption{ACF for the DA and CDA algorithms, and the M-H algorithm with multivariate normal proposal.}
\end{subfigure}
 \caption{Panel (a) demonstrates in traceplot and panel (b) in autocorrelation about the substantial improvement in CDA by correcting the variance mis-match in logistic regression with rare event data, compared with the original DA \citep{polson2013bayesian} and  the M-H algorithm with multivariate normal proposal (MH-MVN).}
    \label{logit_random_mixing}
 \end{figure}



\subsection{General Algorithm}

Before proceeding into theory, we summarize the general algorithm for CDA. We assume the parameters are multi-dimensional and can be divided into two groups $\{ \theta, \tau\}$.  We sample the parameters in $\eta\mid\theta, y$ and the ones in $\theta\mid \tau,y$ alternatively.  For notational ease, we now focus on $\theta$ and omit the conditioning on $\tau$ in the the rest of section. 

Assume $\theta$ can be augmented with latent variable $z$ but is susceptible to slow mixing issue. The total augmented likelihood is the product:

\be \label{eq:da_decomposition}
\prod_i L(m_i(\theta);y_i) =\prod_i   \int \pi\left(m_i(\theta)|z_i,y_i \right)\pi(z_i|y_i) d z_i
\ee
where  $m_i(\theta)$ is a continuous and differentiable function $m_i:\bb R^p \mapsto \bb R^d$, conditioning on which, each integral on the right hand side is independent. For example,  $m_i (\theta) = x_i\theta$ is the linear predictor in regression; $C_i$ is the constant free from $\theta$. Let the the conditional distribution for $z$ and $\theta$ be:

\be
z_i \mid \theta, y &\sim \pi (z_i; m_i(\theta), y)\\
\theta \mid z,y &\sim f( \theta; \mu,\Sigma).
\ee
where $f( \theta; \mu,\Sigma)\propto \pi(\theta) \prod_i \pi\left(m_i(\theta)|z_i,y_i \right)$. To calibrate the variance $\Sigma$, we introduce a parameter $r_i$. When $\Sigma$ is free from $z$, we put $r_i$ in each $\pi\left(m_i(\theta)|z_i,y_i \right)$. When $\Sigma$ involves $z$, we put $r_i$ in $\pi(z_i|y_i)$ to increase $\bb E_z\Sigma$. Then using another parameter $b_i$ to accomodate the shift in $m_i(\theta)$ , we obtain the calibrated data augmentation:

\be \label{eq:cda_decomposition}
\prod_i L_r(m_i(\theta) ; y_i,  r_i, b_i) = \prod_i \int \pi_r\left(m_i(\theta)+b_i|z_i,y_i \right)\pi_r(z_i|y_i) d z_i
\ee

With prior $\pi(\theta)$, the proposal can then be sampled from the calibrated distribution:

\be
z_i & \sim \pi(z_i; m_i(\theta)+b_i, y) \\
\theta^* &\sim f( \theta^*|\mu(r, b),\Sigma(r)).
\ee
in a M-H step with accepting probability:
\be
1 \wedge \prod_i  \frac{L(m_i(\theta^*); y_i) L_r(m_i(\theta) ; y_i)} {L(m_i(\theta); y_i, r_i, b_i) L_r(m_i(\theta^*); y_i, r_i, b_i)}.
\label{eq:mh-criterion}
\ee

We initially run the algorithm by adaptively estimating $r$ and $b$. To choose the value for $r$, we first compute the Fisher information matrix based on the marginal posterior $\pi(\theta|y)= C_L \pi(\theta) \prod_i L(m_i(\theta);y_i)$:

\be
\mc I(\theta) = & \left [  \bb E_y  \left( \frac { \partial \log  \pi(\theta) \prod_i L(m_i(\theta);y_i) } {\partial \theta_{j_1}} \right)  \left( \frac { \partial \log \pi(\theta) \prod_i L(m_i(\theta);y_i)} {\partial \theta_{j_2}} \right) \right ]_{j_1,j_2} \\
= & J' \Diag \left [ \bb E_y \left( \frac { \partial \log L(m_i(\theta);y_i)} {\partial m_{i}(\theta)} \right)  \left( \frac { \partial \log L(m_i(\theta);y_i)} {\partial m_{i}(\theta)} \right) \right]_{i} J + \left [ \left( \frac { \partial \log  \pi(\theta)  } {\partial \theta_{j_1}} \right)\left( \frac { \partial \log  \pi(\theta)  } {\partial \theta_{j_2}} \right)\right ]_{j_1,j_2},
\ee
where $J = [\frac {\partial m_{i}(\theta)}  { \partial \theta_j}]_{i,j}$, and $\Diag$ is the block diagonal matrix (or simple diagonal matrix when all $ m_{i}(\theta)$'s are scalars). Then the Fisher information matrix based on the conditional $  f( \theta|\mu(r, b),\Sigma(r)) =  C_f \pi(\theta) \prod_i \pi_r \left(m_i(\theta)|z_i,y_i \right)$, marginalized over $z$:


\be
\bb E_z \mc I(\theta| z)= & \left [  \bb E_y  \left( \frac { \partial \log  \pi(\theta) \prod_i \pi_r \left(m_i(\theta)|z_i,y_i \right) } {\partial \theta_{j_1}} \right)  \left( \frac { \partial  \pi(\theta) \prod_i \pi_r \left(m_i(\theta)|z_i,y_i \right)} {\partial \theta_{j_2}} \right) \right ]_{j_1,j_2} \\
= & J' \Diag \left [ \bb E_z \bb E_y \left( \frac { \partial \log \pi_r \left(m_i(\theta)|z_i,y_i \right) } {\partial m_{i}(\theta)} \right)  \left( \frac { \partial \log \pi_r\left(m_i(\theta)|z_i,y_i \right) } {\partial m_{i}(\theta)} \right) \right]_{i} J + \left [ \left( \frac { \partial \log  \pi(\theta)  } {\partial \theta_{j_1}} \right)\left( \frac { \partial \log  \pi(\theta)  } {\partial \theta_{j_2}} \right)\right ]_{j_1,j_2},
\ee

Since $r_i$ is in $\pi_r \left(m_i(\theta)|z_i,y_i \right)$, we use it to minimize the difference between 
$ \bb E_y \left( \frac { \partial \log L(m_i(\theta);y_i)} {\partial m_{i}(\theta)} \right)  \left( \frac { \partial \log L(m_i(\theta);y_i)} {\partial m_{i}(\theta)} \right) $
and 
$ \bb E_z \bb E_y \left( \frac { \partial \log \pi_r \left(m_i(\theta)|z_i,y_i \right) } {\partial m_{i}(\theta)} \right)  \left( \frac { \partial \log \pi_r\left(m_i(\theta)|z_i,y_i \right) } {\partial m_{i}(\theta)} \right) $ for all $i$. Due to the conditional independence induced by $m_i(\theta)$,  computing  $r_i$ is often straightforward; and often their difference can be completely removed given $\theta$. Then conditional on $r_i$, we choose $b_i$ to optimize the acceptance rate, one choice is the solution to $L_r(m_i(\theta); y_i,  r_i, b_i)=L(m_i(\theta); y_i)$. After the acceptance rate reaches satisfactory value, we stop tuning $r$ and $b$, and continue in burn-in and posterior sample collecting.

\section{Theory: Mixing Acceleration}

We now study the theory behind acceleration of the mixing after the calibration. Since the posteriors are collected after we stop adaptation, we focus on the period that the parameters $r$ and $b$ are fixed.


The mixing of Markov chain can be described by the geometric convergence rate. Let $\mathcal{P}(\theta,.)$ be the the Markov transition measure and $\pi(.)$ be the target invariant measure and $\theta$ be the state in the state space $\varTheta$. Starting from the initial state $\theta^{(0)}$, the chain is geometrically ergodic if there exist $M: \varTheta \rightarrow [0, \infty)$ and $\rho\in[0,1)$ such that $||\mathcal{P}^k(\theta,.)-\pi(.) ||_{TV} \le M(\theta^{(0)}) \rho^k$, where $||.||_{TV}$ is the total variation distance $|| P_1 -P_2 ||_{TV} = \underset{\mathcal A\in \mathcal F}\sup ||P_1(\mathcal A)-P_2(\mathcal A)||$. As the number of iterations $k\rightarrow \infty$, $||\mathcal{P}^k(\theta,.)-\pi(.) ||_{TV} \rightarrow 0$ leading to convergence to the target. The slow mixing is attributed to $\rho$ being too close to $1$. As shown by \citep{scott2016bayes}, the $\rho$ approaches $1$ as $n$ increases, leading to a complete break-down of algorithm.

We first utilize another related quantity, the norm of the forward operator $||\bf{F}||$, which is defined as  ${\bf F}s(\theta)=\int \mathcal{P}(\theta,\theta') s(\theta') d\theta' = E\{ s(\theta') | \theta \}$. In a Hilbert space $L^2(\pi)=\{s(\theta): \bb E s(\theta)=0, \mbox{var}\{s(\theta)\}<\infty \}$, the norm is defined as the maximal correlation between two states $||{\bf F}||=\underset{s(\theta),t(\theta)\in L^2(\pi)}{\sup}\;\mbox{corr}(s(\theta),t(\theta^{'}))$ \citep{liu2008monte}. This norm is related to $\rho$: when the chain is reversible with detailed balance (e.g. M-H), $\lim_{k\rightarrow \infty}||{\bf F}^k||^{1/k}=\rho$; when the chain is non-reversible, $||{\bf F}||^2$ is equal to the convergence rate of the reversibilized chain \citep{fill1991eigenvalue}.

In each iteration, the original DA samples in the sequence of $\theta' \rightarrow z' \rightarrow \theta \rightarrow z$, where $a\rightarrow b\rightarrow c$ represents $c$ being generated from a conditionally distribution of $b$, and conditionally independent of $a$. Omitting $y$ for simpler notation, by Lemma 4 in \cite{liu1994collapsed}:

\be
||{\bf F}_{DA}|| =\underset{s(\theta)\in L^2(\pi)}{\sup}\; \frac{\mbox{var}_{DA} [ \bb E_{DA} \{ s(\theta,z)|\theta^{'},z'\}]}{\mbox{var}_{DA}\{s(\theta,z) \} }  & = \underset{s(\theta)\in L^2(\pi)}{\sup}\; \frac{\mbox{var}_{DA} [ \bb E_{DA} \{ s(\theta)|z'\}]}{\mbox{var}_{DA}\{s(\theta) \} } \\
& = 1- \underset{s(\theta)\in L^2(\pi)}{\inf}\; \frac{\bb E_{DA}  [  \mbox{var}_{DA}\{ s(\theta)|z'\}]}{\mbox{var}_{DA}\{s(\theta) \} } 
\label{eq:norm_da}
\ee
in which, slow mixing occurs when ${\bb E_{DA}  [  \mbox{var}_{DA}\{ s(\theta)|z'\}]} \ll {\mbox{var}_{DA}\{s(\theta) \} }$.

The calibrated DA samples a little differently: by proposing $\theta^*$ in the calibrated sample and use Metropolis-Hastings to accept the new state $\theta^*$ or keep the previous state $\theta$, it in fact samples in the sequence of $( \theta' , z' )\rightarrow \theta \rightarrow z$, similarly we obtain:

\be
||{\bf F}_{CDA}|| =\underset{s(\theta)\in L^2(\pi)}{\sup}\; \frac{\mbox{var}_{CDA} [ \bb E_{CDA} \{ s(\theta,z)|\theta^{'},z'\}]}{\mbox{var}_{CDA}\{s(\theta,z) \} }  = 1- \underset{s(\theta)\in L^2(\pi)}{\inf}\; \frac{\bb E _{CDA} [  \mbox{var}_{CDA}\{ s(\theta)|z',\theta'\}]}{\mbox{var}_{CDA}\{s(\theta) \} } 
\label{eq:norm_cda}
\ee


To compare the \eqref{eq:norm_da} and \eqref{eq:norm_cda} directly, we rely on the following lemma.

\begin{lemma}
	In Metropolis-Hastings step with current state $\theta'$ and proposal state $\theta^*$ from $f(\theta^*; z')$, if the acceptance probability $p\ge p_0$, the generated state $\theta$ satisfies $\mbox{var}_{CDA}\{ s(\theta)|z', \theta'\} \ge   p_0\cdot \mbox{var}_{CDA} (s(\theta^*)|z')$.
\end{lemma}

Therefore, we can induce an increase in $\bb E [\mbox{var}\{ s(\theta)|z'\}]$ by $\gamma$ times, and obtain the following acceleration:

\begin{theorem}
Let ${\bf F}_{DA}$ and ${\bf F}_{CDA}$ be the forward operators corresponding to the standard DA and the calibrated DA; $\theta$ be the random variable from the DA updating rule and $\theta^*$ be the one from the CDA proposal. Assume the conditional variance increase in the CDA proposal has ${  \bb E [\mbox {var}_{CDA}\{ s(\theta^*)|z,y\}]  } \ge \gamma \cdot { \bb E [\mbox {var}_{DA}\{ s(\theta)|z,y\}]}$ with the Metropolis-Hastings acceptance probability in \eqref{eq:mh-criterion} greater or equal to $p_0>0$. Then if $p_0 \gamma \ge 1$, 
$$||{\bf F}_{CDA}||\le 1- \gamma p_0 \cdot \underset{s(\theta)\in L^2(\pi)}{\inf}\; \frac{\bb E_{DA}  [  \mbox{var}_{DA}\{ s(\theta)|z'\}]}{\mbox{var}_{DA}\{s(\theta) \} } 
\le ||{\bf F}_{DA}||.$$
\end{theorem}

%In practice, $\underset{k=1\ldots p}{\sup}\;\mbox{corr}(\theta_k,\theta^{'}_k) $ is often used as a more tractable approximate to $||\bf F||$. In the probit example, since $z$ is not in the conditional variance, direct increase of $r$ in $\bb E [\mbox {var}_{CDA}\{ s(\theta)|z,y\}] =(X'R^{-1}X)^{-1}$ leads to mixing acceleration. In the logit example, the increase in $\bb E [\mbox {var}_{CDA}\{ s(\theta)|z,y\}] =\bb E(X'ZX)^{-1}$ can be numerically evaluated  (see section 2.2).

As the result, with an increase in $\gamma$ by raising the conditional variance, and acceptance rate $p_0>0$, one can significantly accelerate the mixing. The large $p_0$ is attributed to the similarity between $L_r$ and $L$ in \eqref{eq:mh-criterion}, as the result of adaptation. For example, in the logit CDA:

\be
\alpha_i = \frac{ \{1+\exp(\xbeta)\}   \{1+\exp(\xbeta^*+b_i)\}^{r_i} } {  \{1+\exp(\xbeta^*)\}  \{1+\exp(\xbeta+b_i)\}^{r_i}    } 
\ee
when $\xbeta$ is negative (corresponding to large variance gap that causes slow mixing), the adapted bias reduction term $b_i  = \log\{ 1/r_{i} + O (\frac{\exp(\xbeta)}{r_i} )\} \approx -\log r_i$. Then $\{ 1+\exp(\xbeta-\log r_i)\}^{r_i} = 1+\exp(\xbeta) + O(\frac{\exp(2\xbeta) }{r_i})$.


\section{Real Data Application: Poisson Regression for Online Advertisement Tracking}

We now apply CDA to a real data application in online advertisement tracking. The advertisement is displayed on $n=59,792$ originating websites, pointing to $96$ different targets. The count of click-throughs is recorded for each combination. The counts contain many zeros ($95.5\%$), as not all $96$ advertisements are shown on all the websites. For commercial interests, it is useful  to predict the traffic of the new advertisements using the existing ones. Therefore, we use the data of $95$ advertisements as predictors $x_i$ and the one left as the outcome $y_i$ for a count regression. We use training data collected from a two-week period, and a validation data set collected during another two-week window. 

One common practice to handle the large proportion of zeros is to use zero-inflated Poisson. However, for predictive modeling, this is suboptimal as it would require another set of coefficients to predict the latent binary event, e.g. $y_i\sim p\left( g(x_i\eta)\right)  \delta_0+ \{ 1-p\left( g(x_i \eta) \right) \} Poisson\{\exp (x_i \theta)\}$, while does not solve the slow mixing issue (see appendix). Instead, it is rather useful to consider a simpler model  $y_i\sim Poisson\{\exp(\theta_0+ \sum_j x_{i,j}\theta_j)\}$ with a quite negative intercept $\theta_0$.

It is known that the posterior sampling for Poisson is hindered by slow mixing, which is especially worse with large amount of zeros. The traditional M-H lacks a good strategy to propose multidimensional variables ($p=96$ in this case). There is a Gibbs sampling strategy, first discovered by \citep{zhou2012lognormal} with negative binomial approximation. We further simplify and present the algorithm.

The Poisson density can be viewed as a limit:
\be
L(\xbeta;y)=\frac{ \exp(y_i \xbeta)}{\exp\{\exp(\xbeta)\}y!} =\lim_{\lambda\rightarrow\infty}\frac{\exp(y_i \xbeta)}{\{1+ \exp(\xbeta)/\lambda\}^{\lambda}y!}.
\ee

With large but finite $\lambda$ (e.g. $10,000$), one can sample from the approximate posterior:

\be
z_i \sim & \PG\left (\lambda, \xbeta -\log \lambda\right)\\
\theta \sim & \No \left( \big[ (X' ZX )^{-1} X'  \big ( y - \lambda/2 + z \log \lambda \big) ,(X' Z X )^{-1} \right)
\ee

The problem with this DA is that as $\lambda$ increases, the accuracy is improved but the large $z$ quickly reduces the conditional variance for $\theta$, creating mixing bottleneck. It inevitably becomes a dilemma to trade between accuracy or mixing rate in choosing $\lambda$.

As $\lambda$ control the magnitude of the latent $z$, the calibration is straightforward by replacing $\lambda$ with small $r_i$ and $-\log \lambda$ with $b_i$, giving the calibrated likelihood:

\be
L_r(\xbeta;y)=\frac{\exp \{ y_i (\xbeta + b_i)\}}{\{1+ \exp (\xbeta + b_i)\}^{r_i}},
\ee
which generates the same sampling algorithm as the CDA in logit regression, except that $r_i>y_i$ to ensure the posterior properiety.

We run the approximate DA with large $\lambda=10,000$ and the exact CDA for posterior computation. For CDA, we adapt for $100$ iterations and reaches an acceptance rate of $0.6$. We then run each algorithm for $4,000$ steps and use the last $1,000$ as the posterior sample.

The mixing of DA and CDA is compared in traceplots and autocorrelation plots in Figure~\ref{data_poisson}. DA shows slow mixing for several parameters (Figure\ref{acf_poi_da}), including the important intercept estimate $\theta_0$ (first plot in Figure\ref{traceplot_poi_da}). After calibration, the slow mixing problem is solved:  {\it all} of 96 parameters show very low autocorrelation.

 \begin{figure}[H]
 % \centering
   \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{traceplot_poisson_da}
 \caption{Trace plots of three parameters from DA.}
  \label{traceplot_poi_da}
 \end{subfigure}
  \hfill 
 \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{poisson_da_acf.pdf}
 \caption{Autocorrelation of all the 96 $\theta$'s from DA.}
   \label{acf_poi_da}
 \end{subfigure} 
  \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{traceplot_poisson_cda}
 \caption{Trace plots of three parameters from CDA.}
  \label{traceplot_poi_ada}
 \end{subfigure}
  \hfill 
 \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{poisson_cda_acf.pdf}
 \caption{Autocorrelation of all the 96 $\theta$'s from CDA.}
   \label{acf_poi_ada}
 \end{subfigure}
 \caption{Panels (c) and (d) show significant improvement of the mixing in Poisson data augmentation. Panel (d) show the CDA reduces the autocorrelation for all of the parameters.}
 \label{data_poisson}
 \end{figure}


To empirically evaluate the accuracy of the estimates, we also run Hamiltonian Monte Carlo (HMC) as the reference. HMC is known for its good mixing properties, despite of its costly evaluation. We list the parameter estimates and fit statistics in Table~\ref{table:Poisson}. For simplicity, we include the posterior mean and standard deviation for the intercept $\theta_0$ and the norm of the coefficients $\sum_{j=0}^{95} |\theta_j|$. For goodness-of-fit, we compute root-mean-squared error $RMSE= \sqrt{ \sum_{i=1}^n  (y_i-\mu_i)^2/n}$ and the deviance $D=2\sum_{i=1}^n \{ y_i \log(y_i/\mu_i) -(y_i-\mu_i)\}$, with $\mu_i=\exp( x_i{\hat\theta})$ and ${\hat\theta}$ as the posterior mean. For prediction performance, we use the testing dataset and $\hat y_{i,new}=\exp( x_{i,new}{\hat\theta})$ as the estimator. We evaluate the cross-validation RMSE between $y_{i,new}$ and $\hat y_{i,new}$.

As expected, the estimates for $\theta_0$ from all models are quite negative. However, the DA severely underestimates the variance of the intercept. The coefficient norm also differs greatly from CDA and HMC. Obviously, the poor mixing causes the Markov chain in DA to be trapped in a suboptimal state. After calibration, CDA performs exceptionally well in fit statistics and the validation error that is almost $4$ times lower. The results of CDA and HMC are nearly identical (see appendix for more comparison).

Lastly, it is worth to compare the computing time needed for the three methods. The CDA operates at almost the same cost as the original DA in each iteration. After the extra adaptation period (about $100$ iterations), CDA quickly converges to the target in the first few iterations; whereas DA seems to be stuck even after $2,000$ iterations. HMC is computationally intensive as it requires evaluation of the gradient and multiple Hamiltonian steps in generating a proposal, therefore the speed is about $10$ times slower; and the adaptation in both step size and step number is also time-consuming. In conclusion, CDA is most computationally efficient.


\begin{table}[H]
\centering
\begin{tabular}{|l |r |r| r| r |} 
 \hline
                          & DA & CDA & HMC\\
 [0.5ex]
 \hline
$\theta_0$                         & -4.21 (0.042)& -4.38 (0.075) & -4.47 (0.071) \\
$\sum_{j=0}^{95} |\theta_j|$         & 12.24 (0.10)&  8.58 (0.11)  & 8.68 (0.11)  \\
RMSE                              & 32.86        & 5.06          & 4.88\\
D                                 & 182127.7     & 107076.9      & 106791.3\\
CV-RMSE                           & 32.01        & 8.61          & 8.28\\
Steps for Adaptation \& Burn-in                & 2000         & 100            & 500 \\
Computing Speed (per 1,000 steps)  & 25 mins       & 26 mins        & 300 mins\\
 \hline
\end{tabular}
\caption{Performance of DA, CDA and HMC in Poisson log-linear regression with online advertisement tracking data. Posterior estimates for the intercept and the norm of the coefficients are shown. The CDA shows much improved fit statistics such as root-mean-squared error (RMSE) and deviance (D). In cross-validation (CV-RMSE), the CDA outperforms DA in nearly $4$ times lower in error. The CDA converges much more rapidly than DA. CDA agrees with the HMC very well but takes significantly less time and the adaptation is simpler.}
\label{table:Poisson}
\end{table}


\section{Discussion}

In posterior sampling, when the parameters lack closed-form in the marginal distribution, data augmentation is a useful technique. It has been realized that this practice could severely stall the mixing, due to the gap between the conditional variance with the augmented data and the marginal one. With data size increases and become complex, it is common for the conditional distribution of the parameter to deviate from the area that has reasonable mixing performance. As we show in the previous examples, this quickly leads to an un-manageable increase in the computational time and poor estimation. On the other hand, it is not feasible to directly use the marginals with Metropolis-Hastings, when the parameters are in multi-dimensions, since it is challenging  finding a proposal with the right correlation structure.

To solve this problem, we propose a general class of method to calibrate the variance conditional on the latent variable. With a mechanism to adjust the step size, the transition in each iteration is corrected onto the same order of the marginal variance. The generated samples are used as proposal in the Metropolis-Hastings for exact posterior. In this article, we demonstrate that this strategy is applicable when $\theta \mid z$ belongs to the location-scale family. We expect that it can be extensible to any distribution with a variance / scale, possibly with a different bias-reducing machineary.

There is some similarity between CDA and HMC. Both algorithms excel in seeking proposal with high acceptance rate. The difference is that when the Hamiltonian lacks  closed-form solution (which is mostly true), it requires multiple steps numeric evaluations of the dynamics for one step; whereas CDA only needs one step. Therefore, when the data augmentation exists, CDA is always more preferable.

In this article, we insist on obtaining the exact posterior, to provide a rigorous analysis on the mixing property. Without the Metropolis-Hastings step, the sampling strategy in calibrated data augmentation can be used alone to generate approximate posterior. This can be useful when the evaluation of the marginal likelihood is costly. 

\bibliography{reference}
\bibliographystyle{plainnat}


\section{Appendix}

%
%\subsubsection{Proof of Theorem 1:}
%
%
%Let $\theta=\{\theta_1, \theta_2\}$ be the parameters that are divided into two parts. Let $\theta'$ and $z'$ be the parameters and latent variables in the last iteration. Omitting $y$ for the ease of notation, the square of maximal correlation can be represented as $||{\bf F}||^2=\underset{s(\theta,z),t(\theta,z)\in L^2(\pi)}{\sup}\;\mbox{corr}\{s(\theta,z),t(\theta^{'},z')\}^2
%= \underset{s(\theta,z)\in L^2(\pi)}{\sup}\; \frac{\mbox{var} [ E \{ s(\theta,z)|\theta^{'},z'\}]}{\mbox{var}\{s(\theta,z) \} }$.
%
%The original DA samples in the order of $ \{\theta'_1, \theta'_2\} \rightarrow z' \rightarrow \{\theta_1, \theta_2\}\rightarrow z$, with $ \mbox{var} [ E \{ s(\theta,z)|\theta^{'},z'\}] =  \mbox{var} [ E \{ s(\theta_1, \theta_2,z)|\theta^{'}_1,\theta^{'}_2,z'\}] $. The marginalization and sampling based CDA samples  in the order of $ \theta'_2 \rightarrow z' \rightarrow \theta_2\rightarrow z$, followed by $z' \rightarrow \theta'_1$ and  $z \rightarrow \theta_1$ with $ \mbox{var} [ E \{ s(\theta,z)|\theta^{'},z'\}] =  \mbox{var} [ E \{ s(\theta_1, \theta_2,z)|\theta^{'}_1,\theta^{'}_2,z'\}] = \mbox{var} [ E \{ s(\theta_1, \theta_2,z)|\theta^{'}_2,z'\}]$.
%
%For better clarity, let $E_{X}$ denote the integration over $P(dX)$. 
%
%\begin{equation}
%\begin{aligned}
% \mbox{var} [ E \{ s(\theta_1, \theta_2,z)|\theta^{'}_2,z'\}]  & = E_{\theta^{'}_2,z'}  [ E_{\theta_1, \theta_2,z}\{ s(\theta_1, \theta_2,z)|\theta^{'}_2,z' \} ]^2 -  (E_{\theta^{'}_2,z'}   [ E_{\theta_1, \theta_2,z}\{ s(\theta_1, \theta_2,z)|\theta^{'}_2,z' \} ])^2  \\
% & = E_{\theta^{'}_2,z'}   [ E_{\theta'_1}E_{\theta_1, \theta_2,z} \{ s(\theta_1, \theta_2,z)|\theta'_1, \theta^{'}_2,z' \} ]^2 - (E_{\theta^{'}_1,\theta^{'}_2,z'}   [ E_{\theta_1, \theta_2,z} \{ s(\theta_1, \theta_2,z)|\theta^{'}_1,\theta^{'}_2,z' \} ])^2 \\
%& \le  E_{\theta^{'}_2,z'}  E_{\theta'_1} [ E_{\theta_1, \theta_2,z} \{ s(\theta_1, \theta_2,z)|\theta'_1, \theta^{'}_2,z' \} ]^2 - (E_{\theta^{'}_1,\theta^{'}_2,z'}   [ E_{\theta_1, \theta_2,z} \{ s(\theta_1, \theta_2,z)|\theta^{'}_1,\theta^{'}_2,z' \} ])^2\\
%& =  \mbox{var}  [E \{ s(\theta_1, \theta_2,z)|\theta^{'}_1,\theta^{'}_2,z'\}] 
%\end{aligned}
%\end{equation}
%
%This completes the proof.


\subsection{Proofs}

\subsubsection{Lemma 1}
As the M-H step in CDA is equivalent to sampling from the mixture that:

\be
(1-p)\delta_{\theta'} + p f_{CDA}(\theta^*; z')
\ee
where $p$ is the acceptance probability in \eqref{eq:mh-criterion} and $f_{CDA}$ is the calibrated proposal distribution. Its conditional variance is:

\be
  \mbox{var}_{CDA}\{ s(\theta)|z', \theta'\} = & (1-p) s(\theta')^2 + p \bb E_{CDA} \{ s(\theta^*)^2|z'\}  - [ (1-p) s(\theta') + p \bb E_{CDA} \{ s(\theta^*)|z'\}]^2 \\
  = & (1-p) [s(\theta')^2 - (1-p) s(\theta')^2 - 2p s(\theta')  \bb E_{CDA} \{ s(\theta^*)|z'\}  + p \bb E_f \{ s(\theta^*)|z'\}^2 ] \\
  & + p [\bb E_{CDA} \{ s(\theta^*)^2|z'\} - \bb E_{CDA} \{ s(\theta^*)|z'\}^2]\\
  = & (1-p)p [s(\theta') - \bb E_{CDA} \{ s(\theta^*)|z'\}]^2 + p \cdot \mbox{var}_{CDA} (s(\theta^*)|z') \\
  \ge &  p\cdot \mbox{var}_{CDA} (s(\theta^*)|z') \\
  \ge &  p_0\cdot \mbox{var}_{CDA} (s(\theta^*)|z')
\ee



\subsubsection{Theorem 1}

With Lemma 1,
\be
\bb E [  \mbox{var}_{CDA}\{ s(\theta)|z', \theta'\} ]  
\ge &  p_0 \bb \cdot \bb E [\mbox{var}_{CDA} (s(\theta^*)|z') ]\\
\ge &  p_0 \gamma  \cdot \bb E [\mbox{var}_{DA} (s(\theta^*)|z')].
\ee

Since the marginal variances are the same for two algorithms $\mbox{var}_{DA}\{s(\theta) \} = \mbox{var}_{CDA}\{s(\theta) \}$. When $p_0\gamma \ge 1$, rearranging terms and taking supremum on both sides complete the proof.

%
%\subsubsection{Proof of Theorem 3:}
%
%Without loss of generality, take $M\ge 1$, then $E |\theta_j| {1}_{|\theta_j|>M}\le E \theta_j^2 {1}_{|\theta_j|>M}\le \epsilon_2$.
%
%\begin{equation}
%\begin{aligned}
%|E (\theta_j |y)-E_{r,b} (\theta_j |y)| & = \int  | \theta_j \pi(\theta_j | y) -  \theta_j\pi_{r,b}(\theta_j|y)| d\theta_j  \\
%& \le \int  | \theta_j| |\pi(\theta_j | y) - \pi_{r,b}(\theta_j|y)| d\theta_j  \\
%& =   \int  1(|\theta_j|\le M) | \theta_j|\cdot |\pi(\theta_j | y) - \pi_{r,b}(\theta_j|y)| d\theta_j  +  \int 1(|\theta_j|>M) | \theta_j| |\pi(\theta_j | y) - \pi_{r,b}(\theta_j|y)| d\theta_j  \\
%& \le M   \int   |\pi(\theta_j | y) - \pi_{r,b}(\theta_j|y)| d\theta_j  + \int 1(|\theta_j|>M) | \theta_j| \pi(\theta_j | y) d\theta_j +  \int 1(|\theta_j|>M) | \theta_j| \pi_{r,b}(\theta_j|y) d\theta_j  \\
%& \le 2M\epsilon_1 + 2\epsilon_2 \\
%& = 2M\epsilon_1 + o(\epsilon_1),
%\end{aligned}
%\end{equation}
%where triangle inequality and the definition of total variation distance are used.
%
%
%
%\begin{equation}
%\begin{aligned}
%|\mbox{var} (\theta_j |y)-\mbox{var}_{r,b} (\theta_j |y)| & = | [E (\theta^2_j |y)- \{ E(\theta_j |y)\}^2] - [ E_{r,b} (\theta^2_j |y)-\{ E_{r,b} (\theta_j |y)\}^2 ]|\\
%& \le | E (\theta^2_j |y)-  [ E_{r,b} (\theta^2_j |y) ] |+ | \{ E(\theta_j |y)\}^2 -\{ E_{r,b} (\theta_j |y)\}^2 ]|\\
%& \le 2M^2 \epsilon_1 + 2\epsilon_2 + | \{ E(\theta_j |y)\} -\{ E_{r,b} (\theta_j |y)\} ]| \cdot | \{ E(\theta_j |y)\}+\{ E_{r,b} (\theta_j |y)\}]| \\
%& \le 2M^2 \epsilon_1 + 2\epsilon_2 +  (2M\epsilon_1 + 2\epsilon_2) \{  2 E(\theta_j |y) + 2M\epsilon_1 + 2\epsilon_2 \} \\
%& \le 2M^2 \epsilon_1 + 2\epsilon_2 +  (2M\epsilon_1 + 2\epsilon_2) \{  2 M+ 2\epsilon_2 + 2M\epsilon_1 + 2\epsilon_2 \} \\
%& = 6M^2\epsilon_1 + o(\epsilon_1).
%\end{aligned}
%\end{equation}
%
%
%To prove Corollary 1, using Cauchy-Schwarz inequality $ \{  E\theta_{j_1}\theta_{j_2} 1(\theta_{j_1}>M_{j_1} )  1(\theta_{j_2}>M_{j_2} )    \}^2 \le   E\theta^2_{j_1}1(\theta_{j_1}>M_{j_1} )  E \theta^2_{j_2} 1(\theta_{j_1}>M_{j_2} )   =\epsilon^2_2$. Following the similar proof for variance, it can be derived that:
%
%$$|\mbox{cov}(\theta_{j_1},\theta_{j_2}|y)-\mbox{cov}_{r,b}(\theta_{j_1},\theta_{j_2}|y)|\le 6M_{j_1}M_{j_2}\epsilon_1+o(\epsilon_1 ).$$
%
%\subsubsection{Proof of Theorem 4:}
%
%Since $\theta= B^{-} B\theta$, $\mbox{cov} B\theta= B^{-}  \mbox{cov} B\theta B^{-T} $, applying H\"older's inequality:
%
%$$||{E}\theta-{E}\theta_{r,b}||_1 \le ||B^{-}||_1 ||{E}B\theta- {E}B\theta_{r,b}||_\infty$$
%$$||\mbox{cov}\theta-\mbox{cov}\theta_{r,b}||_1 \le  ||B^{-}||_\infty ||B^{-}\mbox{cov} B\theta- B^{-}\mbox{cov}B\theta_{r,b}||_1 \le ||B^{-}||_1 ||B^{-}||_\infty ||\mbox{cov} B\theta- \mbox{cov}B\theta_{r,b}||_\infty$$
%
%\subsection{Approximation Error in Logistic Regression}
%
%For better clarity, we renumber the double index $ij$ using single index $i$.
%
%\subsubsection{Total Variation Distance}
%
%The individual Kullback-Leibler distance:
%
%\begin{equation}
%\begin{aligned}
%KL\{ { L_{r,b}(y_i|\eta_{i}) } || {L(y_i|\eta_{i})} \}& =\mbox{E}\log \frac{\Gamma(1/r_i+1) r_i^{y_i}  /\Gamma(1/r_i -y_i+1)  }{\Gamma(2) /\Gamma(2 -y_i)  } + \log \frac{ 1+\exp(\eta_{i})}{ \{1+\exp(\eta_{i})r_i\}^{1/r_i}}\\
%& =\log\{1+ \exp ( \eta_{i})\}   - 1/r \log\{1+ r\exp ( \eta_{i})\}\\
%& \le   \{   (r_i-1) \frac{ \exp(2\eta_{i})}{2} \}  1\{\exp(\eta_{i})< 1/r_i\} + \log \frac{ 1+\exp(\eta_{i})}{ \{1+\exp(\eta_{i})r_i\}^{1/r_i}}  1\{\exp(\eta_{i})\ge 1/r_i\} \\
%%& \le   \frac{r_i-1}{2 r_i^2}  1\{r_i <\exp(-\eta_{i}) \} + \log \frac{ 1+\exp(\eta_{i})}{ \{1+\exp(\eta_{i})r_i\}^{1/r_i}}  1\{ \eta_{i} \ge -\log r_i\}.
%\label{KL_logit}
%\end{aligned}
%\end{equation}
%
%
%With adaptive $r_i=1$ if $\eta_{i}\ge -\log r_i$ and Pinsker's inequality,
%
%$$||P_{r,b}(y_i|\eta_{i}) - P(y_i|\eta_{i})||_{TV} \le   \{   \frac{\sqrt{r_{i}-1}  \exp(\eta_{i})}{2} \} 1 \{\eta_{i}< - \log r_{i} \le 0 \}$$
%
%\subsubsection{Tail Integral}
%
%Consider each likelihood $L(y_i|p_i) = p^y_i (1-p)^{1-y_i}$ with $p_i=\frac{\exp(\eta_{i})}{1+\exp(\eta_{i})}$. Applying density transformation leads to 
%$\pi(\eta_{i}|y_i) = \frac{\exp(\eta_{i}) \exp(y_i\eta_{i})}{\{1+\exp(\eta_{i})\}^3}$.
%
%If $y_i=1$,
%\begin{equation}
%	\begin{aligned}
%			\mbox{E}\{ \eta_{i}^2 1(|\eta_{i}|>M) \} & = E\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i} \ge 0 ) \} + E\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i}<0 ) \} \\
%	& \le \int_M^{\infty} \frac{\eta_{i}^2}{1+\exp(\eta_{i})} d\eta_{i} + \int_{-\infty}^{-M}{\eta_{i}^2}{\exp(2\eta_{i})}d\eta_{i}  \\
%	& \le \int_M^{\infty} {\eta_{i}^2}{\exp(-\eta_{i})} d\eta_{i} +\int_{-\infty}^{-M}{\eta_{i}^2}{\exp(2\eta_{i})}d\eta_{i} \\
%	& = (M^2+2M+2)\exp(-M) + \frac{1}{4} (2M^2 + 2M +1) \exp(-2M).
%	\end{aligned}
%\end{equation}
%
%if $y_i=0$,
%\begin{equation}
%	\begin{aligned}
%			\mbox{E}\{ \eta_{i}^2 1(|\eta_{i}|>M) \} & = \mbox{E}\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i} \ge 0 ) \} + E\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i}<0 ) \} \\
%	& \le \int_M^{\infty} \frac{\eta_{i}^2}{\{1+\exp(\eta_{i})\}^2} d\eta_{i} + \int_{-\infty}^{-M}{\eta_{i}^2}{\exp(\eta_{i})}d\eta_{i}  \\
%	& \le \int_M^{\infty} {\eta_{i}^2}{\exp(-2\eta_{i})} d\eta_{i} + \int_{-\infty}^{-M}{\eta_{i}^2}{\exp(\eta_{i})}d\eta_{i} \\
%	& =\frac{1}{4} (2M^2 + 2M +1) \exp(-2M)+  (M^2+2M+2)\exp(-M) .
%	\end{aligned}
%\end{equation}
%
%Therefore, the tail square integral is in $O(M^2 \exp(-M))$.
%
%
%Consider the approximate density $L_{r,b} (y_i| \eta_i) =\frac{\Gamma(1/r_i+1)}{\Gamma(1/r_i-y_i+1)\Gamma(y_i+1)} p^{y_i} (1-p)^{(1/r_i-y_i)}$, where $p=\frac{\exp ( \eta_i+\log r_i)}{\{1+ \exp ( \eta_i +\log r_i)\}}$ and $y_i< 1/r_i+1$.  Applying density transformation leads to $\pi(\eta_{i}|y_i) = \frac{\Gamma(1/r_i+1)}{\Gamma(1/r_i-y_i+1)\Gamma(y_i+1)}\frac{\{r_i\exp(\eta_{i})\}^{(y_i+1)}}{\{1+r_i\exp(\eta_{i})\}^{(1/r_i+2)}}$.
%
%\begin{equation}
%	\begin{aligned}
%			\mbox{E}_{r,b}\{ \eta_{i}^2 1(|\eta_{i}|>M) \}  % = \mbox{E}_{r,b}\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i} \ge 0 ) \} + \mbox{E}_{r,b}\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i}<0 ) \} \\
%	&	 \le      \int      \eta_i^2  1(|\eta_{i}|>M)  \frac{\Gamma(1/r_i+1)r_i^{y_i}}{\Gamma(1/r_i-y_i+1)}\frac{r_i}{\Gamma(y_i+1)}\frac{\{\exp(\eta_{i})\}^{(y_i+1)}}{\{1+r_i\exp(\eta_{i})\}^{(1/r_i+2)}} d\eta_i  \\
%	&  \le \int \eta_i^2  1(|\eta_{i}|>M) \frac{r_i}{y_i!}\frac{\{\exp(\eta_{i})\}^{(y_i+1)}}{\{1+r_i\exp(\eta_{i})\}^{(1/r_i+2)}} d\eta_i  \\	
%		& \le \frac{1}{y_i! r_i^{y_i}} \int_M^{\infty} \frac{\eta_{i}^2}{1+r_i\exp(\eta_{i})} d\eta_{i} +  \frac{r_i}{y_i!} \int_{-\infty}^{-M}{\eta_{i}^2}{\exp\{\eta_{i} (y_i+1) \}}d\eta_{i} \\
%		& \le \frac{1}{y_i! r_i^{y_i+1}} \int_M^{\infty} {\eta_{i}^2}{\exp(-\eta_{i})} d\eta_{i} +  \frac{r_i}{y_i!} \int_{-\infty}^{-M}{\eta_{i}^2}{\exp\{\eta_{i} (y_i+1) \}}d\eta_{i} \\
%	& = \frac{1}{y_i! r_i^{y_i+1}} (M^2+2M+2)\exp(-M) + \frac{r_i}{y_i!}   (M^2+2M+2)\exp(-M) 
%	\end{aligned}
%\end{equation}
%
%\subsection{Approximation Error in Poisson Log-Linear Model}
%
%\subsubsection{Total Variation Distance}
%
%With $\eta_i=\xbeta$, the individual Kullback-Leibler distance:
%
%\begin{equation}
%\begin{aligned}
%KL\{ { L_{r,b}(y_i|\eta_{i}) } || {L(y_i|\eta_{i})} \}& =\mbox{E}\log \frac{\Gamma(1/r_i+1) r_i^{y_i}  }{\Gamma(1/r_i -y_i+1)   } + \log \frac{ \exp\exp(\eta_{i})}{ \{1+\exp(\eta_{i})r_i\}^{1/r_i}}\\
%& \le \exp ( \eta_{i})   - 1/r \log\{1+ r\exp ( \eta_{i})\}\\
%& \le   \{   r_i \frac{ \exp(2\eta_{i})}{2} \}  1\{\exp(\eta_{i})< 1/r_i\} + \log \frac{ \exp\exp(\eta_{i})}{ \{1+\exp(\eta_{i})r_i\}^{1/r_i}} 1\{\exp(\eta_{i})\ge 1/r_i\} \\
%\label{KL_poisson}
%\end{aligned}
%\end{equation}
%
%With adaptive $r_i = 0$ if $\eta_{i}\ge 1/r_i$ and Pinsker's inequality,
%$$||P_{r,b}(y_i|\eta_{i}) - P(y_i|\eta_{i})||_{TV} \le   \{   \frac{\sqrt{r_{i}}  \exp(\eta_{i})}{2} \} 1 \{\eta_{i}< - \log r_{i} \}$$
%
%
%
%\subsubsection{Tail Integral}
%
%
%Consider each likelihood $L(y_i|p_i) = {p_i^{y_i}\exp(-p_i)}/{y_i!}$ with $p_i={\exp(\eta_{i})}$. Applying density transformation leads to 
%$\pi(\eta_{i}|y_i) = \exp \{\eta_i (y_i+1)\} \exp\{-\exp(\eta_i) \} /{y_i!}$. Without loss of generality, assume $|M|\ge 1$.
%
%
%\begin{equation}
%	\begin{aligned}
%			E\{ \eta_{i}^2 1(|\eta_{i}|>M) \} & = E\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i} \ge 0 ) \} + E\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i}<0 ) \} \\
%	& \le \int_M^{\infty}  \frac{ \exp \{\eta_i (y_i+3)\}  \} }{\exp\{\exp(\eta_i)\} e^2 y_i!} d\eta_{i} + \int_{-\infty}^{-M}  \frac {\eta_{i}^2 \exp \{ \eta_{i} (y_i+1)\}  }{ y_i !}d\eta_{i}  \\
%	& =  \frac{IGamma(y_i+3, \exp(M)\} }{e^2 y_i!} +   \frac{IGamma(3, (y_i+1)M)\} }{(y_i+1)^3 y_i!} .
%	\end{aligned}
%\end{equation}
%where $IGamma(a,b)$ is the incomplete Gamma function $\int_b^{\infty} t^{a-1} \exp(-t) dt$, equal to the $\{1-F(b) \} \Gamma(a)$, with $F(b)$ as the cumulative distribution function of gamma distribution ${\mathcal G}(a,1)$.
%
%
%Similar to the logistic approximate, consider the approximate density $L_{r,b} (y_i| \eta_i) =\frac{\Gamma(1/r_i+1)}{\Gamma(1/r_i-y_i+1)\Gamma(y_i+1)} p^{y_i} (1-p)^{(1/r_i-y_i)}$, where $p=\frac{\exp ( \eta_i+\log r_i)}{\{1+ \exp ( \eta_i +\log r_i)\}}$ and $y_i< 1/r_i+1$.  Applying density transformation leads to $\pi(\eta_{i}|y_i) = \frac{\Gamma(1/r_i+1)}{\Gamma(1/r_i-y_i+1)\Gamma(y_i+1)}\frac{\{r_i\exp(\eta_{i})\}^{(y_i+1)}}{\{1+r_i\exp(\eta_{i})\}^{(1/r_i+2)}}$.
%
%
%Note when $\eta_i>0$ hence $r_i < 1/exp(0)=1$, $\frac{1}{1+r_i\exp(\eta_{i})\}^{(1/r_i)}}\le \frac{1}{1+ \exp(\eta_{i})} $. Then,
%
%\begin{equation}
%	\begin{aligned}
%			\mbox{E}_{r,b}\{ \eta_{i}^2 1(|\eta_{i}|>M) \}  % = \mbox{E}_{r,b}\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i} \ge 0 ) \} + \mbox{E}_{r,b}\{ \eta_{i}^2 1(|\eta_{i}|>M, \eta_{i}<0 ) \} \\
%	&	 \le      \int      \eta_i^2  1(|\eta_{i}|>M)  \frac{\Gamma(1/r_i+1)r_i^{y_i}}{\Gamma(1/r_i-y_i+1)}\frac{r_i}{\Gamma(y_i+1)}\frac{\{\exp(\eta_{i})\}^{(y_i+1)}}{\{1+r_i\exp(\eta_{i})\}^{(1/r_i+2)}} d\eta_i  \\
%	&  \le \int \eta_i^2  1(|\eta_{i}|>M) \frac{r_i}{y_i!}\frac{\{\exp(\eta_{i})\}^{(y_i+1)}}{\{1+r_i\exp(\eta_{i})\}^{(1/r_i+2)}} d\eta_i  \\	
%		& \le \int_M^{\infty} \frac{r_i}{y_i! r_i^{y_i+1}}\frac{\eta_i^2}{\{1+r_i\exp(\eta_{i})\}} d\eta_i 
%				+  \frac{r_i}{y_i!} \int_{-\infty}^{-M}{\eta_{i}^2}{\exp\{\eta_{i} (y_i+1) \}}d\eta_{i} \\
%	& =  \frac{1}{y_i! r_i^{(y_i+1)}}   (M^2+2M+2)\exp(-M) + \frac{r_i}{y_i!}    \frac{IGamma(3, (y_i+1)M)\} }{(y_i+1)^3 }
%	\end{aligned}
%\end{equation}



\subsection{Mixing of Zero-inflated Poisson without Calibration}


 \begin{figure}[H]
 % \centering
   \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{traceplot_poisson_zip_da.pdf}
 \caption{Trace plots of three parameters from DA ZIP model}
 \end{subfigure}
  \hfill 
 \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{poisson_zip_da_acf.pdf}
 \caption{Autocorrelation of all the 96 $\theta$'s from DA ZIP model.}
 \end{subfigure}  
 \caption{The hierarchy in the zero-inflated Poisson model does NOT help reduce the autocorrelation.}
 \end{figure}



\subsection{Goodness-of-Fit and Cross-Validation for Poisson Regression}


 \begin{figure}[H]
 % \centering
   \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{poisson_fitting_da.png}
 \caption{Fitted vs true values using DA}
 \end{subfigure}
  \hfill 
 \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{poisson_fitting_ada.png}
 \caption{Fitted vs true values using CDA}
 \end{subfigure}  
   \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{poisson_cv_da.png}
 \caption{Prediction vs true values using DA}
 \end{subfigure}
  \hfill 
 \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{poisson_cv_ada.png}
 \caption{Prediction vs true values using CDA}
 \end{subfigure} 
 \caption{The posterior estimates produced by CDA is better fitted to the data and have more accurate prediction than DA.}
 \end{figure}

 \subsection{Comparing posterior samples of CDA with HMC}


\begin{figure}[H]
 % \centering
   \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{CDAvsHMC_mean.pdf}
 \caption{Comparing posterior means for $\theta_1,\dots \theta_{95}$ from the HMC and CDA. The  RMSE between the two is $0.0007$.}
 \end{subfigure}
  \hfill 
 \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{CDAvsHMC_sd.pdf}
 \caption{Comparing posterior standard deviation for $\theta_1,\dots \theta_{95}$ from the HMC and CDA.  The  RMSE between the two is $0.0004$.}
 \end{subfigure}  
 \caption{The results from CDA and HMC agree very well.}
 \end{figure}

\subsection{Mixing of HMC}


 \begin{figure}[H]
 % \centering
   \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{traceplot_poisson_hmc.pdf}
 \caption{Traceplots}
 \end{subfigure}
  \hfill 
 \begin{subfigure}[b]{0.45\textwidth}
 \includegraphics[width=1\textwidth]{poisson_hmc_acf.pdf}
 \caption{Autocorrelation}
 \end{subfigure}  
 \caption{The posterior estimates produced by HMC.}
 \end{figure}
 


 
\end{document}




