\relax 
\citation{ngai2011application}
\citation{wakefield2007disease}
\citation{wang2010click}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{minsker2014robust,srivastava2015wasp,conrad2015accelerating}
\citation{roberts2004general,meyn2012markov}
\citation{rajaratnam2015mcmc}
\citation{hairer2011asymptotic}
\citation{hairer2014spectral}
\citation{johndrow2016inefficiency}
\citation{albert1993bayesian}
\citation{polson2013bayesian}
\citation{hairer2014spectral}
\citation{tanner1987calculation,albert1993bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {2}Calibrated Data Augmentation}{3}}
\newlabel{eq:da}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Initial example: Probit with improper prior}{3}}
\citation{liu1999parameter}
\citation{meng1999seeking}
\citation{johndrow2016inefficiency}
\newlabel{eq:prop-marginal-probit}{{2}{4}}
\newlabel{eq:cda-probit}{{3}{4}}
\newlabel{eq:mh-criterion}{{4}{4}}
\citation{johndrow2016inefficiency}
\citation{albert1993bayesian}
\newlabel{eq:varlb-probit}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Imbalanced data intercept only case}{5}}
\citation{albert1993bayesian}
\citation{liu1999parameter}
\citation{albert1993bayesian}
\citation{liu1999parameter}
\citation{albert1993bayesian}
\citation{liu1999parameter}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{probit_demo_intercept_proposal}{{1a}{6}}
\newlabel{sub@probit_demo_intercept_proposal}{{a}{6}}
\newlabel{probit_demo_intercept_density}{{1b}{6}}
\newlabel{sub@probit_demo_intercept_density}{{b}{6}}
\newlabel{probit_demo_intercept_posteriorsample}{{1c}{6}}
\newlabel{sub@probit_demo_intercept_posteriorsample}{{c}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Autocorrelation functions (ACFs) and kernel-smoothed density estimates for different CDA samplers in intercept-only probit model.\relax }}{6}}
\newlabel{probit_demo_intercept}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Choice of calibration parameters}{6}}
\citation{polson2013bayesian}
\newlabel{probit_reg_trace}{{2a}{7}}
\newlabel{sub@probit_reg_trace}{{a}{7}}
\newlabel{probit_reg_acf}{{2b}{7}}
\newlabel{sub@probit_reg_acf}{{b}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Panel (a) demonstrates in traceplot and panel (b) in autocorrelation the substantial improvement in CDA by correcting the variance mis-match in probit regression with rare event data, compared with the original \citep  {albert1993bayesian} and parameter-expanded methods \citep  {liu1999parameter}.\relax }}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Logistic regression example: A second calibration approach}{7}}
\citation{cressie1981moment}
\citation{polson2013bayesian}
\citation{polson2013bayesian}
\citation{polson2013bayesian}
\citation{polson2013bayesian}
\newlabel{eq:prop-marginal-logit}{{6}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Panel (a) demonstrates in traceplot and panel (b) in autocorrelation the substantial improvement of CDA in logistic regression with rare event data, compared with the original DA \citep  {polson2013bayesian} and the M-H algorithm with multivariate normal proposal (MH-MVN).\relax }}{9}}
\newlabel{logit_random_mixing}{{3}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}General Algorithm}{9}}
\newlabel{eq:da_decomposition}{{7}{9}}
\newlabel{eq:cda_decomposition}{{8}{9}}
\citation{liu2008monte}
\citation{fill1991eigenvalue}
\citation{liu1994collapsed}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theory: Mixing Acceleration}{10}}
\citation{papaspiliopoulos2007general}
\citation{yang2013sequential}
\newlabel{eq:norm_da}{{9}{11}}
\newlabel{eq:norm_cda}{{10}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Co-Browsing Behavior Application}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Hierarchical Binomial Model for Estimating Co-browsing Rates}{12}}
\citation{polson2013bayesian}
\citation{carpenter2016stan}
\citation{polson2013bayesian}
\citation{polson2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Boxplots of the ACFs show the mixing of the $59,792$ parameters in the hierarchical binomial model, for the original DA\citep  {polson2013bayesian}, CDA and HMC.\relax }}{13}}
\newlabel{data_binomial}{{4}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parameter estimates (with 95\% credible intervals) and effective sample sizes ($T_{eff}$) of the DA, CDA and HMC in hierarchical binomial model. CDA provides parameter estimates as accurate as HMC, and is more computationally efficient than HMC.\relax }}{13}}
\newlabel{tab:binomial}{{1}{13}}
\citation{zhou2012lognormal}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Poisson Log-Normal Model for Web Traffic Prediction}{14}}
\newlabel{eq:pos_approx}{{11}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces CDA significantly improves the mixing of the parameters in the Poisson log-normal.\relax }}{16}}
\newlabel{data_poisson}{{5}{16}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Parameter estimates, prediction error and computing speed of the DA, CDA and HMC in Poisson regression model. Compared with HMC, CDA shows similar performance in both parameter estimation and prediction, but is about 5 times faster.\relax }}{16}}
\newlabel{table:Poisson}{{2}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{16}}
\bibdata{reference}
\bibcite{albert1993bayesian}{{1}{1993}{{Albert and Chib}}{{}}}
\bibcite{carpenter2016stan}{{2}{2016}{{Carpenter et~al.}}{{Carpenter, Gelman, Hoffman, Lee, Goodrich, Betancourt, Brubaker, Guo, Li, and Riddell}}}
\bibcite{conrad2015accelerating}{{3}{2015}{{Conrad et~al.}}{{Conrad, Marzouk, Pillai, and Smith}}}
\bibcite{cressie1981moment}{{4}{1981}{{Cressie et~al.}}{{Cressie, Davis, Folks, and Folks}}}
\bibcite{fill1991eigenvalue}{{5}{1991}{{Fill}}{{}}}
\bibcite{hairer2011asymptotic}{{6}{2011}{{Hairer et~al.}}{{Hairer, Mattingly, and Scheutzow}}}
\bibcite{hairer2014spectral}{{7}{2014}{{Hairer et~al.}}{{Hairer, Stuart, Vollmer, et~al.}}}
\bibcite{johndrow2016inefficiency}{{8}{2016}{{Johndrow et~al.}}{{Johndrow, Smith, Pillai, and Dunson}}}
\bibcite{liu1994collapsed}{{9}{1994}{{Liu}}{{}}}
\bibcite{liu2008monte}{{10}{2008}{{Liu}}{{}}}
\bibcite{liu1999parameter}{{11}{1999}{{Liu and Wu}}{{}}}
\bibcite{meng1999seeking}{{12}{1999}{{Meng and Van~Dyk}}{{}}}
\bibcite{meyn2012markov}{{13}{2012}{{Meyn and Tweedie}}{{}}}
\bibcite{minsker2014robust}{{14}{2014}{{Minsker et~al.}}{{Minsker, Srivastava, Lin, and Dunson}}}
\bibcite{ngai2011application}{{15}{2011}{{Ngai et~al.}}{{Ngai, Hu, Wong, Chen, and Sun}}}
\bibcite{papaspiliopoulos2007general}{{16}{2007}{{Papaspiliopoulos et~al.}}{{Papaspiliopoulos, Roberts, and Sk{\"o}ld}}}
\bibcite{polson2013bayesian}{{17}{2013}{{Polson et~al.}}{{Polson, Scott, and Windle}}}
\bibcite{rajaratnam2015mcmc}{{18}{2015}{{Rajaratnam and Sparks}}{{}}}
\bibcite{roberts2004general}{{19}{2004}{{Roberts et~al.}}{{Roberts, Rosenthal, et~al.}}}
\bibcite{srivastava2015wasp}{{20}{2015}{{Srivastava et~al.}}{{Srivastava, Cevher, Tran-Dinh, and Dunson}}}
\bibcite{tanner1987calculation}{{21}{1987}{{Tanner and Wong}}{{}}}
\bibcite{wakefield2007disease}{{22}{2007}{{Wakefield}}{{}}}
\bibcite{wang2010click}{{23}{2010}{{Wang et~al.}}{{Wang, Li, Cui, Zhang, and Mao}}}
\bibcite{yang2013sequential}{{24}{2013}{{Yang and Dunson}}{{}}}
\bibcite{zhou2012lognormal}{{25}{2012}{{Zhou et~al.}}{{Zhou, Li, Dunson, and Carin}}}
\bibstyle{plainnat}
\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Proofs}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Lemma 1}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Theorem 1}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Goodness-of-Fit and Cross-Validation for Poisson Regression}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The posterior estimates produced by CDA is better fitted to the data and have more accurate prediction than DA.\relax }}{20}}
